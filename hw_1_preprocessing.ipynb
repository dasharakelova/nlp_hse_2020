{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymystem3 import Mystem\n",
    "from rusenttokenize import ru_sent_tokenize\n",
    "from razdel import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/zhivago.txt')\n",
    "data = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear(text):\n",
    "    no_tags = re.sub(r'<[^>]+>', ' ', text)\n",
    "    no_newline = re.sub(r'\\n', '', no_tags) # тут будут проблемы со строками без пунктуации в конце типа заголовков, но ими мы пожертвуем:(\n",
    "    no_nbrspaces = re.sub(r'\\xa0', ' ', no_newline)\n",
    "    no_spaces = re.sub('  +', ' ', no_nbrspaces)\n",
    "    return no_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_data = clear(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2\n",
    "\n",
    "Я не поняла, как получится разделить текст на предложения, если мы уберем знаки пунктуации. Так что убирать пунктуацию будем только при токенизации, в предложениях она важна:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenized = ru_sent_tokenize(cleared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [sent.lower() for sent in sent_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = ['.',',','!','?',':',';','«','»','–', '-', '...', '(', ')', '„', '“']\n",
    "tokens = [token.text.lower() for token in list(tokenize(cleared_data)) if token.text not in punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_sents = [sent for sent in set(sents) if sents.count(sent) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Да, в тексте 132 таких предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('андреевич', 289)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter([w for w in tokens if len(w) > 6]).most_common(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Это токен \"андреевич\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = [(word, stemmer.stem(word)) for word in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем найти возможные ошибки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif_words = []\n",
    "no_stemming = []\n",
    "for pair in set(stems):\n",
    "    for pair1 in set(stems):\n",
    "        if pair[0] != pair1[0] and pair[1] == pair1[1]:\n",
    "            dif_words.append((pair, pair1))\n",
    "    if pair[0] == pair[1]:\n",
    "        no_stemming.append(pair)\n",
    "    if len(dif_words) >=20 and len(no_stemming) >= 60:\n",
    "        break     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) \"платы\" и \"платьями\" (основа *плат-*), \"шипит\" и \"шипах\" (основа *шип-*), \"окажемся\" и \"окажите\" (основа *окаж-*)\n",
    "\n",
    "2) \"плывут\", \"нейдет\", \"сжав\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно добавить туда больше вопросительных слов (*почему*, *откуда*), т.к. они обычно не несут особенной смысловой нагрузки, если только речь не идет о художественном приеме; еще там явно не хватает союза *который*. Имеет смысл занести в список вводные слова, которые нужны только для связности текста, например, *итак* или *во-первых*.\n",
    "\n",
    "А еще не очень понятно, почему некоторые слова даны в списке только в начальной форме, а некоторые в разных. Если применять список к не-лемматизированным словам, то нужно дополнить многие парадигмы (*моего*, *кого*, *какого*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing mystem to C:\\Users\\Daria Arakelova/.local/bin\\mystem.exe from http://download.cdn.yandex.net/mystem/mystem-3.0-win7-64bit.zip\n"
     ]
    }
   ],
   "source": [
    "mystem = Mystem()\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas1 = [morph.parse(token)[0].normal_form for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas2 = [l for l in mystem.lemmatize(cleared_data.lower()) if l.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сравнить работу лемматизаторов на отрывке текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "борис | борис\n",
      "леонид | леонидович\n",
      "пастернак | пастернак\n",
      "доктор | доктор\n",
      "живаго | живаго\n",
      "доктор | доктор\n",
      "живаго | живаго\n",
      "итоговый | итоговый\n",
      "произведение | произведение\n",
      "борис | борис\n",
      "пастернак | пастернак\n",
      "книга | книга\n",
      "весь | весь\n",
      "он | его\n",
      "жизнь | жизнь\n",
      "этот | этот\n",
      "роман | роман\n",
      "принести | приносить\n",
      "он | он\n",
      "автор | автор\n",
      "мировой | мировой\n",
      "известность | известность\n",
      "и | и\n",
      "нобелевский | нобелевский\n",
      "премия | премия\n",
      "присуждение | присуждение\n",
      "который | который\n",
      "обернуться | обертываться\n",
      "для | для\n",
      "поэт | поэт\n",
      "оголтелый | оголтелый\n",
      "политический | политический\n",
      "травля | травля\n",
      "обвинение | обвинение\n",
      "в | в\n",
      "измена | измена\n",
      "родина | родина\n",
      "и | и\n",
      "в | в\n",
      "результат | результат\n",
      "стоить | стоить\n",
      "он | он\n",
      "жизнь | жизнь\n",
      "доктор | доктор\n",
      "живаго | живаго\n",
      "роман | роман\n",
      "сам | сам\n",
      "ткань | ткань\n",
      "который | который\n",
      "убедительный | убедительно\n",
      "свидетельствовать | свидетельствовать\n",
      "о | о\n",
      "чудо | чудо\n",
      "чем | чем\n",
      "весь | весь\n",
      "размышление | размышление\n",
      "доктор | доктор\n",
      "и | и\n",
      "обобщение | обобщение\n",
      "автор | автор\n",
      "человек | человек\n",
      "который | который\n",
      "так | так\n",
      "писать | писать\n",
      "бесконечно | бесконечно\n",
      "много | много\n",
      "пережить | переживать\n",
      "и | и\n",
      "передумать | передумывать\n",
      "и | и\n",
      "главное | главный\n",
      "он | его\n",
      "чувство | чувство\n",
      "на | на\n",
      "свет | свет\n",
      "восхитить | восхищенный\n",
      "умиление | умиление\n",
      "и | и\n",
      "слёзный | слезный\n",
      "сострадание | сострадание\n",
      "конечно | конечно\n",
      "есть | быть\n",
      "в | в\n",
      "он | его\n",
      "мир | мир\n",
      "место | место\n",
      "и | и\n",
      "презрение | презрение\n",
      "и | и\n",
      "холодное | холодный\n",
      "отстранение | отстранение\n",
      "но | но\n",
      "не | не\n",
      "в | в\n",
      "они | они\n",
      "быть | суть\n",
      "роман | роман\n",
      "пастернак | пастернак\n",
      "оплакивание | оплакивание\n",
      "прежний | прежний\n",
      "заблуждение | заблуждение\n",
      "и | и\n",
      "они | их\n",
      "жертва | жертва\n",
      "тот | тот\n",
      "кто | кто\n",
      "не | не\n",
      "разделять | разделять\n",
      "молитвенный | молитвенный\n",
      "восторг | восторг\n",
      "перед | перед\n",
      "мир | мир\n",
      "достойный | достойный\n",
      "прежде | прежде\n",
      "всего | все\n",
      "жалость | жалость\n",
      "перечитывать | перечитывать\n",
      "доктор | доктор\n",
      "живаго | живаго\n",
      "стоить | стоять\n",
      "именно | именно\n",
      "тогда | тогда\n",
      "когда | когда\n",
      "кажется | казаться\n",
      "что | что\n",
      "жить | жить\n",
      "не | не\n",
      "стоить | стоить\n",
      "тогда | тогда\n",
      "десять | десять\n",
      "строка | строка\n",
      "из | из\n",
      "это | этот\n",
      "роман | роман\n",
      "мочь | мочь\n",
      "сделать | сделать\n",
      "то | то\n",
      "же | же\n",
      "что | что\n",
      "делать | делать\n",
      "любовь | любовь\n",
      "в | в\n",
      "один | один\n",
      "из | из\n",
      "стихотворение | стихотворение\n",
      "доктор | доктор\n",
      "жизнь | жизнь\n",
      "вернуться | вернуться\n",
      "так | так\n",
      "же | же\n"
     ]
    }
   ],
   "source": [
    "for i in range(150):\n",
    "    print(lemmas1[i] + ' | ' + lemmas2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По этому небольшому отрывку текста кажется, что mystem лучше справляется с задачей лемматизации. Вот его достоинства по сранению с pymorphy:\n",
    "- Он правильно обрабатывает притяжятельное местоимение *его* (pymorphy приводит к личному *он*)\n",
    "- Он нечувствителен к виду глагола, что можно считать достоинством, если мы хотим составить словарь текста. Хотя в каких-то других случаях, наверное, лучше выделять глаголы совершенного вида в самостоятельные леммы, как это делает pymorphy\n",
    "- Он обрабатывает причастия как отдельные от глагола леммы, что справедливо с точки зрения их синтаксической дистрибуции\n",
    "- Pymorphy считает, что *суть* -- это форма глагола *быть*, что очень мило, но непрактично\n",
    "\n",
    "При этом mystem неправильно разрешает омонимию в слове *стоит*. В целом кажется, что анализ mystem-а не такой глубокий, как у pymorphy, но он лучше для каких-то прикладных задач."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

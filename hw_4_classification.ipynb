{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2**\n",
    "\n",
    "а)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5, max_df=0.4)\n",
    "X = vectorizer.fit_transform(data.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50771793]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(X[3], X[12666])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответ: 0.50771793\n",
    "\n",
    "б)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1957 Че за бригада и че за махоун? Из полицейской академии? [[0.23279247]]\n",
      "14325 Ну оль, ну вот че тебе надо? Иди в по сри, а этот достопочтенный тред оставь в покое\n",
      " [[0.25324249]]\n",
      "2245 Че ты там мямлишь, утырок? Сопли подотри\n",
      " [[0.27934598]]\n",
      "43 Люди зажрались и охуели если по мнению этих игроков андромеда лучше Антема. Хотя че там с багаутом76 сравнивают вон... Вот оно че оказывается, игроки просто охуели, ну ок. Вот долбанные пидерасы, не хотят покупать такую отличную игру, а еще смеют ругать такой божественный геймплей. Баги, хуевый геймплей поправят, а контент, ну его запилят, через год другой, причем бесплатно! Ага. Стоит лишь потерпеть!\n",
      " [[1.]]\n"
     ]
    }
   ],
   "source": [
    "for i in cosine_similarity(X[43], X).argsort()[0, -4:]:\n",
    "    print(i, data.loc[i, 'comment'], cosine_similarity(X[43], X[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле эти тексты не очень близки к тексту 43. Но у них и значения близости невысокое, так что, видимо, у 43 текста просто нет сильно похожих в датасете.\n",
    "\n",
    "**Задание 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.1, shuffle=True)\n",
    "train.reset_index(inplace=True)\n",
    "test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим два классификатора: логистическую регрессию с tf-idf векторами и наивный байес с векторами абсолютных значений частотности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.89      0.90       985\n",
      "         1.0       0.77      0.81      0.79       457\n",
      "\n",
      "    accuracy                           0.86      1442\n",
      "   macro avg       0.84      0.85      0.85      1442\n",
      "weighted avg       0.87      0.86      0.87      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer1 = TfidfVectorizer(strip_accents='unicode', ngram_range=(1,2), min_df=5, max_df=0.4, binary=True)\n",
    "X = vectorizer1.fit_transform(train.comment)\n",
    "X_test = vectorizer1.transform(test.comment)\n",
    "y = train.toxic.values\n",
    "y_test = test.toxic.values\n",
    "\n",
    "clf1 = LogisticRegression(C=1, class_weight='balanced')\n",
    "clf1.fit(X, y)\n",
    "preds = clf1.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.91      0.92       985\n",
      "         1.0       0.81      0.85      0.83       457\n",
      "\n",
      "    accuracy                           0.89      1442\n",
      "   macro avg       0.87      0.88      0.87      1442\n",
      "weighted avg       0.89      0.89      0.89      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(strip_accents='unicode', max_df=0.5, min_df=0.0001, analyzer='word', ngram_range=(1,1))\n",
    "X = vectorizer2.fit_transform(train.comment)\n",
    "X_test = vectorizer2.transform(test.comment)\n",
    "y = train.toxic.values\n",
    "y_test = test.toxic.values\n",
    "\n",
    "clf2 = MultinomialNB(alpha=0.5, fit_prior=False)\n",
    "clf2.fit(X, y)\n",
    "preds = clf2.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь предскажем токсичность для 3000 текстов из корпуса двача и выведем десять самых токсичных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2ch_corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    dvach = f.readlines()[:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первым логрег-классификатором:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_dvach_1 = vectorizer1.transform(dvach)\n",
    "probas1 = clf1.predict_proba(X_test_dvach_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Ты превращаешься в Суисейсеки, десу.\\n',\n",
       " ' >ElixirНу ты и говноед.\\n',\n",
       " ' Крысу ты разделовал?\\n',\n",
       " ' Ебать ты юморист донный.\\n',\n",
       " ' Кто подданный? Ты подданый, ёпт. Здравствуй.\\n',\n",
       " ' Просто ты скучный и предсказуемый.\\n',\n",
       " ' Ты автор - тебе видней.\\n',\n",
       " ' И нахуй ты это ему написал?\\n',\n",
       " ' Ты зацени какой подклад на пиджаке\\n',\n",
       " '   остановись, ты пишешь хуйню\\n']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tox_probs = list(zip([x for x in range(len(probas1))], [x[1] for x in probas1]))\n",
    "toxic_ids = [x[0] for x in sorted(tox_probs, key=lambda x: x[1], reverse=True)[:10]]\n",
    "texts = [dvach[i] for i in toxic_ids]\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И вторым байесовским классификатором:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_dvach_2 = vectorizer2.transform(dvach)\n",
    "probas2 = clf2.predict_proba(X_test_dvach_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Ёбаные советские названия блять. КРУЖКИ блять. Всегда ненавидел. Блять. Почему бы не сказать КЛУБ ПО ИНТЕРЕСАМ, как у япошек, нет блять, будем гуманитарную хуйню, сравнения блять, типа В КРУГ СОБРАЛИСЬ, ахахахаха кружок ахахахахаха))))) пиздец блять. Может при Сталине это звучало, но уже при Горбачёве это просто бесило, а сейчас это выглядит вообще как атавизм. Или ещё ебанутое слово СЛЁТ блять. Сука, СЛЁТ, почему слёт, а не съезд? Типа съезд - это для КПСС, не доросли ещё? Или типа в Совке так всё пиздато, что даже у школия есть своё самолёт? СЛЁТ сука, мы что, блять, стрижи? Или это какая-то аллегория на ёбаных ОРЛЯТ? Вот тоже заебучее сравнение, всегда бесило, ОРЯЛА УЧАТСЯ ЛЕТАТЬ, блять, да мне похуй на каких-то куриц, орлят, блять, голубей, петухов, учатся они летать блять, а страусы вон не учатся, мне-то что до ваших сраных ОРЛЯТ, пел всегда КОЛЗЛЯТ, да-да, школие, этот прикол существовал ещё до Задорнова и прочих клованов - в совковых песнях заменять \"орёл\" на \"козёл\". Ну до чего же сука поганая страна была, хорошо, что развалилась нахуй!\\n',\n",
       " ' - Что можете сказать о стиле Тарантиныча?- Кого блять?- Квентина Тарантино.- Так и говори, блять.- Почему его героев, говорящих обо всём и ни о чём, так приятно слушать?- Потому что диалоги остроумные. Из двух слов. Умные. И острые. - Как лист салата?- Что \"как лист салата\"?- Умные.- Что за хуйню ты сейчас несешь?- Мы же сейчас о тарантиновских диалогах с тобой разговариваем?- Да, блять, о них мы и разговариваем. Ты кокаином обнюханный что ли я не пойму?- При чем здесь кокаин? Его диалоги без штампов, без стереотипов, и точно бьющие в суть. Пусть это и суть гамбургера. - Какого, мать твою, гамбургера? Ты еще скажи, что они, блять, свежие как лист салата. Гавно для наркоманов - твоё тарантино. Первые два фильма нормальные. Бешенные псы и Криминальное чтиво..- И чего же блять нормального в Криминальном чтиве? Я, если хочешь знать, героин попробовал из-за него.- И что, жалеешь?- Нет.- Пидора ответ.- Лол блять)- Ты что, на дваче сидишь.- Нет- Пидора ответ- Лол блять\\n',\n",
       " ' Блять, вот что значит каникулы. Ты на издаче, ублюдок ты ебаный, тут пишут грамотно, или хотя бы стараются. Ты что за бред тут высрал, я не понял даже половины, какие адовые, какой миниус, какой Челленджер, какие блять единицы, ты про что вообще несешь, еб твою мать?\\n',\n",
       " ' >Сначала, если ещё не читал, почитай русскую классику типа Достоевского, Чехова, Пушкина. >Потом советскую литературу Стругацких там, ещё кого-то. Че несешь-то?Бля, не слушай его. Я читал и Достоевского, и Чехова, и Пушкина. Даже Стругацких читал. Перечитал уйму всякого говна, даже Улисса за каким-то хуем прочел. И вот что я тебе скажу - чтение не имеет никакого отношения к писательству. И сколько бы ты книг ни прочел, это не даст тебе ничего, кроме мнээээ скилла быстрочтения. Конечно, читать стоит, если тебе нравится эта хуйня. Но только ради этого, а не какого-то сомнительного соображения, что это научит тебя писать. Если хочешься научиться писать - садись и пиши.\\n',\n",
       " ' А знаете, я расскажу одну историю. У меня, пару лет назад, когда я был школьником, был сосед. Владиком звать. История типичная: омега, травля, всратый и молчаливый. Я с ним пытался сдружиться, но он был пассивен к этому. Его мать удивлялась, мол ПОЧЕМУ МОЮ КОРЗИНКУ НИКТО НЕ ЛЮБИТ, НИКТО НЕ ДРУЖИТ С НИМ. Но с ним никакого смысла дружить нет! Он не может поддержать разговор, шутит тупые шутки и страшный на вид из-за усов. Это всё я веду к тому, что тебе, ОП не стоит удивляться отсутствию круга общения, если ты имеешь СЛОЖНЫЙ характер и не можешь вписаться ни в какой круг общения. Меня самого регулярно травили до 7 класса. Но я выбрался из этого дерьма, отпиздив с дикой злости одного травителя. Потом научился относительно неплохо шутить, за счёт чего поднялся до... гаммы. ОП, делай вывод с первого параграфа. Ливаю с треда.\\n',\n",
       " ' А теперь почитайте что пишут всякие пезды и охуейте такой секси мм Пиздец. Какая-то марамойка в своих мечтах представляет что он увидит её камент, бросит свою пизду, заберет шлюху из Рахи и будет пялить. А она ему борщи и личиноккоторых он ненавидит будет рожать.\\n',\n",
       " ' Иди нахуй дурачек ебаный пидорас ну чтоьты за скотина ебаная оп выше написал что нашел ее а теперь иди нахкй\\n',\n",
       " ' >Scala, Clojure, Erlang, Elixir, Common Lisp, Haskell, Ocaml, F#, Elm, Swift, Go, Rust, D, Nim, Scheme, SmalltalkМолодец, перечислил сложные для старта языки + на которых вакансий хуй хуй да нихуя. Сам-то с какого языка начинал? И наверное в гарварде и mit дауны сидят и учат людей сям и питону, сука на дваче, что не доска, так куча нитаких, как все даунов на хайпе, которые толкают свое говно. Нахуй иди.\\n',\n",
       " ' Как олдфаг 29лвл, заставший конец двача, прошедший весь педальчан, и вкусивший сполна абучана, могу заверить: раньше аудиория была другой. Особенно круто было на дваче и раннем педальчане, ибо преобладала студентота преимущественно из технарей, гики, задроты, анимешники, поехавшие, и прочие интересные личности. Конечно, было дохера тредов про \"двач я посрал\", или тупые перефорсы с упячки, форчана, да даже с удаффа залетали и срали. Но это было иначе, чувствовалось, что говно, которое ты кушал в б, оно до переваривания было изысканными трюфелями, а не как сейчас, словно дошик запитый балтикой переварили и высрали. Ну или более точный пример: представь, что препод не пришел на пару, и вам предстоит почти час сидеть страдать хуйней. Так вот. 2ch. ru - это куражащиеся студенты бауманки. А харкач - это куражащиеся ПТУ-шники. Я уж не говорю, как же пиздат был вконтакт в 2007, когда там сидели онли всякие задроты, и где тян было найти, как на аиб, нечто удивительное. Как старательно люди тогда подходили к созданию сообществ, словно к маленькому уютному кружку по интересам. А сейчас поглядите на этот пиздец. Уже зашквар там сидеть. Впрочем, тут тоже зашквар уже сидеть, просто ты не понимаешь этого, как не понимает 16-летняя девочка просиживающая часами в соцсетях, то нынешний вконтакт для даунов.\\n',\n",
       " '  мамка твоя мне заработала своей пиздой вонючей тварь вонючая, мразь поганая, гандон, говно, пидрила, ану иди сюда\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tox_probs = list(zip([x for x in range(len(probas2))], [x[1] for x in probas2]))\n",
    "toxic_ids = [x[0] for x in sorted(tox_probs, key=lambda x: x[1], reverse=True)[:10]]\n",
    "texts = [dvach[i] for i in toxic_ids]\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты получились совсем разные: во-первых, бросается в глаза, что логрег выбрал короткие тексты, а байес - больше длинных. Даже не знаю, как это можно объяснить. Во-вторых, тексты из второго набора гораздо больше подходят под описание токсичности, чем тексты из первого, хотя и в первом такие встречаются; возможно, это связано с тем, что второй классификатор показал лучшие результаты на тестовой выборке и в целом получился точнее."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
